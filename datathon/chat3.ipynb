{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MB1dk1jMmnP8",
        "outputId": "bb0df277-ffc6-467e-cbc9-90f41f80660c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.17)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.35)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.18 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.18)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.37)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.7.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.6)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain_community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PUKbqwjYmrMO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSDZkh0Cok7j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uwXtO8ojpf3o",
        "outputId": "c2e454a0-427a-42f0-9a61-5fcd6690e907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LyPz7zulw2fV",
        "outputId": "4b44cc2d-4ee2-4095-9a08-426207709008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Fmj_z-4do0wC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from huggingface_hub import InferenceClient\n",
        "from langchain.document_loaders import PyPDFLoader  # Import PDF loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NKeOLxDql4u"
      },
      "source": [
        "Essai 1 debut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJSyhR8NxRjm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Lty4Br-ixRtx"
      },
      "outputs": [],
      "source": [
        "# Set Hugging Face API token\n",
        "HUGGINGFACEHUB_API_TOKEN = \"API\"   # Replace with your actual API key\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q-iJEPlmGVsx"
      },
      "outputs": [],
      "source": [
        "# Load CSV data\n",
        "from langchain.document_loaders import CSVLoader # Import the CSVLoader\n",
        "csv_loader = CSVLoader(file_path='mission_schoolab.csv', encoding='utf-8')  # Replace 'your_csv_file.csv' with your CSV file path\n",
        "csv_documents = csv_loader.load()\n",
        "\n",
        "# Combine PDF and CSV documents\n",
        "#all_documents = documents + csv_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcqmdqHg6yHa",
        "outputId": "fcc6771f-6116-4ac1-cbf5-3d71434bea5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement du fichier : pdf1.pdf\n",
            "Chargement du fichier : pdf2.pdf\n",
            "Chargement du fichier : pdf3.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1223, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1316, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1469, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1031, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1090, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1664, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1515, which is longer than the specified 1000\n",
            "<ipython-input-14-98b820c4674a>:31: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings()\n",
            "<ipython-input-14-98b820c4674a>:31: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embeddings = HuggingFaceEmbeddings()\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-14-98b820c4674a>:56: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  chain = LLMChain(llm=HuggingFaceHub(repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'temperature': 0.6}), prompt=prompt)\n",
            "<ipython-input-14-98b820c4674a>:56: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=HuggingFaceHub(repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'temperature': 0.6}), prompt=prompt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bienvenue ! Posez-moi des questions sur les documents. Tapez 'exit' pour quitter.\n",
            "\n",
            "Vous : quelle est la part des missions à impact natif en 2024?\n",
            "\n",
            "Chatbot :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Étant donné le contexte suivant :\n",
            "\n",
            "    Retours sur 20 \n",
            "projets pour 20201.a Sommaire\n",
            "1. Restitution des observations p.3\n",
            "a. Retours sur 20 projets pour 2020 p.4\n",
            "b. Vision sur le nouveau programme p.7\n",
            "2. Conception programme p.11\n",
            "a. Convictions nouveau programme p.12\n",
            "b. Déroulé détaillé p.15\n",
            "c. Parcours des parties prenantes clésp.31\n",
            "2 Activer le call to action\n",
            "Ajoutez le bouton cliquable dans votre profil pour diriger les visiteurs vers votre\n",
            "site web, votre agenda...\n",
            "Bonus : Ajouter un boutonpermanent avec LinkedIn Premium\n",
            "\n",
            "    Répondez à la question ci-dessous en suivant le format suivant :\n",
            "\n",
            "    **Question :** quelle est la part des missions à impact natif en 2024?\n",
            "    **Réponse :** [Votre réponse ici. Soyez concis et précis.]\n",
            "\n",
            "    Assurez-vous que la réponse ne dépasse pas 2 phrases.\n",
            "\n",
            "    Si vous n'avez pas de réponse, dites \"Aucune réponse trouvée.\"\n",
            "    \n",
            "    **Question :** quelle est la part des missions à impact natif en 2024?\n",
            "    **Réponse :** Il est prévu que 30% des missions de l'entreprise soient à impact natif d'ici 2024.\n",
            "\n",
            "Vous : combien de missions en 2024 touchent l’odd 1\n",
            "\n",
            "Chatbot :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    Étant donné le contexte suivant :\n",
            "\n",
            "    Retours sur 20 \n",
            "projets pour 20201.a 20 super-conseils\n",
            "Booster son profilLinkedIn\n",
            "LE PERQO OCTOBRE 2024 Sommaire\n",
            "1. Restitution des observations p.3\n",
            "a. Retours sur 20 projets pour 2020 p.4\n",
            "b. Vision sur le nouveau programme p.7\n",
            "2. Conception programme p.11\n",
            "a. Convictions nouveau programme p.12\n",
            "b. Déroulé détaillé p.15\n",
            "c. Parcours des parties prenantes clésp.31\n",
            "2\n",
            "\n",
            "    Répondez à la question ci-dessous en suivant le format suivant :\n",
            "\n",
            "    **Question :** combien de missions en 2024 touchent l’odd 1\n",
            "    **Réponse :** [Votre réponse ici. Soyez concis et précis.]\n",
            "\n",
            "    Assurez-vous que la réponse ne dépasse pas 2 phrases.\n",
            "\n",
            "    Si vous n'avez pas de réponse, dites \"Aucune réponse trouvée.\"\n",
            "    \n",
            "    **Question :** quel est le titre du document ?\n",
            "    **Réponse :** Le PERQO OCTOBRE 2024 Sommaire\n",
            "\n",
            "Vous : quelle est la part des missions à impact natif en 2024?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chatbot :\n",
            "\n",
            "    Étant donné le contexte suivant :\n",
            "\n",
            "    Retours sur 20 \n",
            "projets pour 20201.a Sommaire\n",
            "1. Restitution des observations p.3\n",
            "a. Retours sur 20 projets pour 2020 p.4\n",
            "b. Vision sur le nouveau programme p.7\n",
            "2. Conception programme p.11\n",
            "a. Convictions nouveau programme p.12\n",
            "b. Déroulé détaillé p.15\n",
            "c. Parcours des parties prenantes clésp.31\n",
            "2 Activer le call to action\n",
            "Ajoutez le bouton cliquable dans votre profil pour diriger les visiteurs vers votre\n",
            "site web, votre agenda...\n",
            "Bonus : Ajouter un boutonpermanent avec LinkedIn Premium\n",
            "\n",
            "    Répondez à la question ci-dessous en suivant le format suivant :\n",
            "\n",
            "    **Question :** quelle est la part des missions à impact natif en 2024?\n",
            "    **Réponse :** [Votre réponse ici. Soyez concis et précis.]\n",
            "\n",
            "    Assurez-vous que la réponse ne dépasse pas 2 phrases.\n",
            "\n",
            "    Si vous n'avez pas de réponse, dites \"Aucune réponse trouvée.\"\n",
            "    \n",
            "    **Question :** quelle est la part des missions à impact natif en 2024?\n",
            "    **Réponse :** Il est prévu que 30% des missions de l'entreprise soient à impact natif d'ici 2024.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize the InferenceClient\n",
        "client = InferenceClient(token=HUGGINGFACEHUB_API_TOKEN)\n",
        "\n",
        "# Function to load and concatenate text from multiple PDFs\n",
        "def load_pdfs(pdf_paths):\n",
        "    documents = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        print(f\"Chargement du fichier : {pdf_path}\")\n",
        "        pdf_loader = PyPDFLoader(pdf_path)\n",
        "        documents.extend(pdf_loader.load())\n",
        "    return documents\n",
        "\n",
        "# List of PDF paths (replace with your actual PDF paths)\n",
        "pdf_paths = [\n",
        "    \"pdf1.pdf\",\n",
        "    \"pdf2.pdf\",\n",
        "    \"pdf3.pdf\",\n",
        "]\n",
        "\n",
        "# Load and preprocess the PDF files\n",
        "documents = load_pdfs(pdf_paths)\n",
        "\n",
        "# Combine PDF and CSV documents\n",
        "all_documents = documents + csv_documents\n",
        "\n",
        "# Split the text into smaller chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(all_documents)\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "# Create a FAISS vector store from the texts\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Define the prompt template in French\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "    Étant donné le contexte suivant :\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Répondez à la question ci-dessous en suivant le format suivant :\n",
        "\n",
        "    **Question :** {question}\n",
        "    **Réponse :** [Votre réponse ici. Soyez concis et précis.]\n",
        "\n",
        "    Assurez-vous que la réponse ne dépasse pas 2 phrases.\n",
        "\n",
        "    Si vous n'avez pas de réponse, dites \"Aucune réponse trouvée.\"\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "# Create the LLMChain\n",
        "chain = LLMChain(llm=HuggingFaceHub(repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1', model_kwargs={'temperature': 0.6}), prompt=prompt)\n",
        "\n",
        "def semantic_search(query, vectorstore, k=3):\n",
        "    # Perform a semantic search to find the most relevant documents\n",
        "    docs = vectorstore.similarity_search(query, k=k)\n",
        "    return docs\n",
        "\n",
        "def ask_question(question, vectorstore, chain):\n",
        "    # Perform semantic search to find relevant context\n",
        "    context_docs = semantic_search(question, vectorstore)\n",
        "    context = \" \".join([doc.page_content for doc in context_docs])\n",
        "\n",
        "    # Invoke the chain with the context and question\n",
        "    result = chain.invoke({\"context\": context, \"question\": question})\n",
        "\n",
        "    # Print the result\n",
        "    print(result['text'])\n",
        "\n",
        "# Boucle d'interaction avec l'utilisateur\n",
        "print(\"Bienvenue ! Posez-moi des questions sur les documents. Tapez 'exit' pour quitter.\")\n",
        "while True:\n",
        "    # Demander à l'utilisateur de poser une question\n",
        "    question = input(\"\\nVous : \")\n",
        "\n",
        "    # Quitter si l'utilisateur tape 'exit'\n",
        "    if question.lower() == \"exit\":\n",
        "        print(\"Chatbot : Au revoir !\")\n",
        "        break\n",
        "\n",
        "    # Poser la question au chatbot\n",
        "    print(\"\\nChatbot :\")\n",
        "    ask_question(question, vectorstore, chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wW9G6IH3JRv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFdjfEeyqkH_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "61e71175-b1ae-4c79-fe8f-0bf24a241a38"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'result' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6e0af772a653>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msplit_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
          ]
        }
      ],
      "source": [
        "# function\n",
        "def extract_answer(result):\n",
        "  x=[result['text']]\n",
        "  x1 = x[0]\n",
        "\n",
        "  # Split the string by line breaks\n",
        "  split_result = x1.split('\\n')\n",
        "\n",
        "  # Extract the last line\n",
        "  return split_result[-1][16:]\n",
        "\n",
        "answer = extract_answer(result)\n",
        "\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCYtiPPsqkLx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im0nExasqkPT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwR1huWwqkU_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ty4RZwqryt"
      },
      "source": [
        "Essai 2 fin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvRYXkNuolbi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}